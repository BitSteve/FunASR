model: MinMo_S2T
model_conf:
  lsm_weight: 0.1
  length_normalized_loss: true
audio_encoder: /cpfs_speech/zhifu.gzf/init_model/SenseVoiceSANM
audio_encoder_conf:
  hub: ms
  freeze: true
  freeze_layer_num: -1
llm: Qwen2.5-7B-Instruct
llm_conf:
  hub: hf
  freeze: true
  llm_dtype: bf16
  init_param_path: /cpfs_speech/zhifu.gzf/init_model/qwen/Qwen2.5-7B-Instruct
  use_lora: true
  lora_conf:
    task_type: "CAUSAL_LM"
    r: 16
    lora_alpha: 32
    lora_dropout: 0.05
    bias: "none"
    target_modules:
      - q_proj
      - v_proj
audio_adaptor: Transformer
audio_adaptor_conf:
  freeze: true
  downsample_rate: 2
  ffn_dim: 2048
  llm_dim: 5120
  encoder_dim: 1280
  n_layer: 1
frontend: WhisperFrontend
frontend_conf:
  fs: 16000
  n_mels: 128
  do_pad_trim: false
  filters_path: /cpfs_speech/zhifu.gzf/init_model/SenseVoiceSANM/assets/mel_filters.npz
train_conf:
  use_lora: ${llm_conf.use_lora}
  accum_grad: 8
  grad_clip: 5
  max_epoch: 2
  keep_nbest_models: 100
  log_interval: 50
  effective_save_name_excludes:
  - llm.
  resume: true
  validate_interval: 10000
  save_checkpoint_interval: 10000
  avg_nbest_model: 100
  use_bf16: false
  use_deepspeed: false
  deepspeed_config: /nfs/zhifu.gzf/codebase/FunASR/examples/deepspeed_conf/ds_stage1.json
  save_init_model: false
optim: adamw
optim_conf:
  lr: 0.00003
  weight_decay: 0.0
scheduler: warmuplr
scheduler_conf:
  warmup_steps: 10000
dataset: OpenAIDatasetMultiTurn
dataset_conf:
  index_ds: OpenAIIndexDSJsonl
  batch_sampler: BatchSampler
  batch_type: token
  batch_size: 1500
  max_token_length: 1500
  shuffle: true
  sort_size: 512
  batch_size_scale_ratio_max: 2
  num_workers: 4
  audio_adaptor_downsample_rate: ${audio_adaptor_conf.downsample_rate}
  audio_encoder_downsample_rate: 4
  data_split_num: 512
  batch_size_sample_max: 15
  retry: 50
  batch_size_token_max: 4000
  max_source_length: 5500
tokenizer: HuggingfaceTokenizer
tokenizer_conf:
  init_param_path: ${llm_conf.init_param_path}
enable_tf32: true
debug: false
excludes: llm.
